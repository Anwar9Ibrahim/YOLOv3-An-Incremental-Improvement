{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpuXkI15nZ6l"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "id": "wpuXkI15nZ6l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7AoD1YporIT"
      },
      "outputs": [],
      "source": [
        "%cd drive/MyDrive/Yolov3"
      ],
      "id": "i7AoD1YporIT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fItdLFc2pCIg"
      },
      "outputs": [],
      "source": [
        "!ls"
      ],
      "id": "fItdLFc2pCIg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKb9JqZRsSYu"
      },
      "outputs": [],
      "source": [
        "! unzip archive.zip"
      ],
      "id": "bKb9JqZRsSYu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP_2KoYPsKwG"
      },
      "outputs": [],
      "source": [
        "#install data augmentation library\n",
        "!pip install -U albumentations"
      ],
      "id": "fP_2KoYPsKwG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ahfpQtuWGQK"
      },
      "source": [
        "##1- import needed libraries "
      ],
      "id": "9ahfpQtuWGQK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-H-5kV4scIa",
        "outputId": "0688a4fa-7f15-48f8-86e7-3fe2a19da48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1020, 0.3021, 0.7511, 0.0174, 0.0273, 0.0673, 0.0010, 0.0046, 0.0080])\n"
          ]
        }
      ],
      "source": [
        "import albumentations as A\n",
        "import cv2\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from intersection_over_union import intersection_over_union , intersection_over_union_wh\n",
        "from mean_average_precision import mean_average_precision\n",
        "from non_max_suppression import nms\n",
        "from YOLOV3_the_model import YOLOv3\n",
        "from tqdm import tqdm\n",
        "## import the utils\n",
        "from yolov3_loss_function import Yolov3Loss\n",
        "from yolo_dataset import YOLODataset"
      ],
      "id": "I-H-5kV4scIa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8h3Geq1WNY8"
      },
      "source": [
        "##2- define hyper and nessecery parameters"
      ],
      "id": "p8h3Geq1WNY8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "145b4e2e-49b7-4fb2-ba63-c6aa9dd98d3c"
      },
      "outputs": [],
      "source": [
        "#to get some performance improvments\n",
        "torch.backends.cudnn.benchmark = True\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#the anchors that was calculated using k-means\n",
        "ANCHORS = [\n",
        "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
        "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
        "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
        "]  # Note these have been rescaled to be between [0, 1]\n",
        "#this number of classes id for the pascal dataset for coco it is 80\n",
        "NUM_CLASSES = 20\n",
        "LEARNING_RATE = 1e-5\n",
        "WEIGHT_DECAY = 1e-4\n",
        "NUM_EPOCHS = 10\n",
        "#if the probability of an object is greater than 0.05 then we say there is an objct\n",
        "#in the bbx\n",
        "CONF_THRESHOLD = 0.05\n",
        "#this is used to calculate the mean average preciision\n",
        "MAP_IOU_THRESH = 0.5\n",
        "NMS_IOU_THRESH=0.45\n",
        "#the dataset\n",
        "DATASET = 'PASCAL_VOC'\n",
        "\n",
        "#check if we wanna load the dataset\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = True\n",
        "SAVE_MODEL = True\n",
        "\n",
        "#where we will save the models\n",
        "CHECKPOINT_FILE = \"checkpoint.pth.tar\"\n",
        "filename= \"checkpoint.pth.tar\"\n",
        "IMG_DIR = DATASET + \"/images/\"\n",
        "LABEL_DIR = DATASET + \"/labels/\"\n",
        "NUM_WORKERS = 2 #4\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 416\n",
        "#scaler to scale images\n",
        "S = [IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8] #13 26 52\n",
        "scale = 1.1\n",
        "#pascal classes\n",
        "\n",
        "PASCAL_CLASSES = [\n",
        "    \"aeroplane\",\n",
        "    \"bicycle\",\n",
        "    \"bird\",\n",
        "    \"boat\",\n",
        "    \"bottle\",\n",
        "    \"bus\",\n",
        "    \"car\",\n",
        "    \"cat\",\n",
        "    \"chair\",\n",
        "    \"cow\",\n",
        "    \"diningtable\",\n",
        "    \"dog\",\n",
        "    \"horse\",\n",
        "    \"motorbike\",\n",
        "    \"person\",\n",
        "    \"pottedplant\",\n",
        "    \"sheep\",\n",
        "    \"sofa\",\n",
        "    \"train\",\n",
        "    \"tvmonitor\"\n",
        "]\n",
        "\n",
        "scaled_anchors = (\n",
        "        torch.tensor(ANCHORS)\n",
        "        * torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "    ).to(DEVICE)\n"
      ],
      "id": "145b4e2e-49b7-4fb2-ba63-c6aa9dd98d3c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmUY_aJishSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b45363-3f80-49f2-d0d0-b98e219f759f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/albumentations/imgaug/transforms.py:346: FutureWarning: This IAAAffine is deprecated. Please use Affine instead\n",
            "  warnings.warn(\"This IAAAffine is deprecated. Please use Affine instead\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "train_transforms = A.Compose(\n",
        "    [\n",
        "     #Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.\n",
        "     A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n",
        "     #adding pad if the if side is less than desired number\n",
        "     A.PadIfNeeded(\n",
        "         min_height=int(IMAGE_SIZE * scale),\n",
        "         min_width=int(IMAGE_SIZE * scale),\n",
        "         border_mode=cv2.BORDER_CONSTANT,\n",
        "         ),\n",
        "     #Crop a random part of the input.\n",
        "     A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),\n",
        "     #Randomly changes the brightness, contrast, and saturation of an image\n",
        "     A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
        "     A.OneOf(\n",
        "         [\n",
        "          #randomly translate, scale and rotate the input.\n",
        "          A.ShiftScaleRotate(rotate_limit=10, p=0.4, border_mode=cv2.BORDER_CONSTANT),\n",
        "          #Place a regular grid of points on the input and randomly move the neighbourhood of these point around\n",
        "          A.IAAAffine(shear=10, p=0.4, mode=\"constant\"),\n",
        "          ],p=1.0,),\n",
        "     A.HorizontalFlip(p=0.5),\n",
        "     A.Blur(p=0.1),\n",
        "     #Apply Contrast Limited Adaptive Histogram Equalization to the input image\n",
        "     A.CLAHE(p=0.1),\n",
        "     #Reduce the number of bits for each color channel.\n",
        "     A.Posterize(p=0.1),\n",
        "     A.ToGray(p=0.1),\n",
        "     #Randomly rearrange channels of the input RGB image.\n",
        "     A.ChannelShuffle(p=0.05),\n",
        "     A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "     ToTensorV2(),],\n",
        "     bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
        ")\n",
        "test_transforms = A.Compose(\n",
        "    [\n",
        "        A.LongestMaxSize(max_size=IMAGE_SIZE),\n",
        "        A.PadIfNeeded(\n",
        "            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT\n",
        "        ),\n",
        "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
        ")\n"
      ],
      "id": "PmUY_aJishSp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43af2126-ace2-4441-93f6-68e66bdc9943"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
        "  loop = tqdm(train_loader, leave=True)\n",
        "  losses = []\n",
        "  for batch_idx, (x, y) in enumerate(loop):\n",
        "      x = x.to(DEVICE)\n",
        "      y0, y1, y2 = (\n",
        "          y[0].to(DEVICE),\n",
        "          y[1].to(DEVICE),\n",
        "          y[2].to(DEVICE),\n",
        "      )\n",
        "\n",
        "      with torch.cuda.amp.autocast():\n",
        "          out = model(x)\n",
        "          loss = (\n",
        "              loss_fn(out[0], y0, scaled_anchors[0])\n",
        "              + loss_fn(out[1], y1, scaled_anchors[1])\n",
        "              + loss_fn(out[2], y2, scaled_anchors[2])\n",
        "          )\n",
        "\n",
        "      losses.append(loss.item())\n",
        "      optimizer.zero_grad()\n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "      # update progress bar\n",
        "      mean_loss = sum(losses) / len(losses)\n",
        "      loop.set_postfix(loss=mean_loss)"
      ],
      "id": "43af2126-ace2-4441-93f6-68e66bdc9943"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c3a060a"
      },
      "outputs": [],
      "source": [
        "#this function returns dataloaders(weither train loaders or test loaders) for a specific dataset using yoloDataSet python code\n",
        "def get_loaders(train_csv_path, test_csv_path):\n",
        "  IMAGE_SIZE = 416\n",
        "  train_dataset = YOLODataset(\n",
        "      train_csv_path,\n",
        "      transform=train_transforms,\n",
        "      S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "      img_dir=IMG_DIR,\n",
        "      label_dir=LABEL_DIR,\n",
        "      anchors=ANCHORS,\n",
        "  )\n",
        "  test_dataset = YOLODataset(\n",
        "      test_csv_path,\n",
        "      transform= test_transforms,\n",
        "      S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "      img_dir=IMG_DIR,\n",
        "      label_dir=LABEL_DIR,\n",
        "      anchors=ANCHORS,\n",
        "  )\n",
        "  train_loader = DataLoader(\n",
        "      dataset=train_dataset,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      num_workers=NUM_WORKERS,\n",
        "      pin_memory=PIN_MEMORY,\n",
        "      shuffle=False,\n",
        "      drop_last=False,\n",
        "  )\n",
        "  test_loader = DataLoader(\n",
        "      dataset=test_dataset,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      num_workers=NUM_WORKERS,\n",
        "      pin_memory=PIN_MEMORY,\n",
        "      shuffle=False,\n",
        "      drop_last=False,\n",
        "  )\n",
        "\n",
        "  train_eval_dataset = YOLODataset(\n",
        "      train_csv_path,\n",
        "      transform=test_transforms,\n",
        "      S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "      img_dir=IMG_DIR,\n",
        "      label_dir=LABEL_DIR,\n",
        "      anchors=ANCHORS,\n",
        "  )\n",
        "  train_eval_loader = DataLoader(\n",
        "      dataset=train_eval_dataset,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      num_workers=NUM_WORKERS,\n",
        "      pin_memory=PIN_MEMORY,\n",
        "      shuffle=False,\n",
        "      drop_last=False,\n",
        "  )\n",
        "\n",
        "  return train_loader, test_loader, train_eval_loader"
      ],
      "id": "8c3a060a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anyrhPBhApuV"
      },
      "outputs": [],
      "source": [
        "def check_class_accuracy(model, loader, threshold):\n",
        "  model.eval()\n",
        "  tot_class_preds, correct_class = 0, 0\n",
        "  tot_noobj, correct_noobj = 0, 0\n",
        "  tot_obj, correct_obj = 0, 0\n",
        "\n",
        "  for idx, (x, y) in enumerate(tqdm(loader)):\n",
        "      x = x.to(DEVICE)\n",
        "      with torch.no_grad():\n",
        "          out = model(x)\n",
        "\n",
        "      for i in range(3):\n",
        "          y[i] = y[i].to(DEVICE)\n",
        "          obj = y[i][..., 0] == 1 # in paper this is Iobj_i\n",
        "          noobj = y[i][..., 0] == 0  # in paper this is Iobj_i\n",
        "\n",
        "          correct_class += torch.sum(\n",
        "              torch.argmax(out[i][..., 5:][obj], dim=-1) == y[i][..., 5][obj]\n",
        "          )\n",
        "          tot_class_preds += torch.sum(obj)\n",
        "\n",
        "          obj_preds = torch.sigmoid(out[i][..., 0]) > threshold\n",
        "          correct_obj += torch.sum(obj_preds[obj] == y[i][..., 0][obj])\n",
        "          tot_obj += torch.sum(obj)\n",
        "          correct_noobj += torch.sum(obj_preds[noobj] == y[i][..., 0][noobj])\n",
        "          tot_noobj += torch.sum(noobj)\n",
        "\n",
        "  print(f\"Class accuracy is: {(correct_class/(tot_class_preds+1e-16))*100:2f}%\")\n",
        "  print(f\"No obj accuracy is: {(correct_noobj/(tot_noobj+1e-16))*100:2f}%\")\n",
        "  print(f\"Obj accuracy is: {(correct_obj/(tot_obj+1e-16))*100:2f}%\")\n",
        "  model.train()"
      ],
      "id": "anyrhPBhApuV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "927LF49wA8Mw"
      },
      "outputs": [],
      "source": [
        "#This function scales the predictions from the output of the model so it would be relative for the input image (helps with plotting)\n",
        "def cells_to_bboxes(predictions, anchors, S, is_preds=True):\n",
        "#   This function takes as parametes:\n",
        "#     1-predictions: tensor of size (N, 3, S, S, num_classes+5)\n",
        "#     2-anchors: used in predictions phase\n",
        "#     3-S the number of cells the image is divided in on the width (and height)\n",
        "#     4-is_preds: wether the input is predictions or ground_truth bboxes (which means it can be used to with data directly from the dataSet)\n",
        "#   And returns Converted_bboxes which is converted bboxes with sizes (N, num_anchors, S, S, 1+5) , the number 6 is [class_index, Object_score,bbox cordinates]\n",
        "  BATCH_SIZE = predictions.shape[0]\n",
        "  num_anchors = len(anchors)\n",
        "  box_predictions = predictions[..., 1:5]\n",
        "  if is_preds:\n",
        "      anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
        "      box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
        "      box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n",
        "      scores = torch.sigmoid(predictions[..., 0:1])\n",
        "      best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
        "  else:\n",
        "      scores = predictions[..., 0:1]\n",
        "      best_class = predictions[..., 5:6]\n",
        "\n",
        "  cell_indices = (\n",
        "      torch.arange(S)\n",
        "      .repeat(predictions.shape[0], 3, S, 1)\n",
        "      .unsqueeze(-1)\n",
        "      .to(predictions.device)\n",
        "  )\n",
        "  x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n",
        "  y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n",
        "  w_h = 1 / S * box_predictions[..., 2:4]\n",
        "  converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n",
        "  return converted_bboxes.tolist()"
      ],
      "id": "927LF49wA8Mw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d82f720-c8ee-48a5-be4d-4686e5b79474"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  model = YOLOv3(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "  optimizer = torch.optim.Adam(\n",
        "      model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "  )\n",
        "  loss_fn = Yolov3Loss()\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  train_loader, test_loader, train_eval_loader = get_loaders(\n",
        "      train_csv_path=DATASET + \"/train.csv\", test_csv_path=DATASET + \"/test.csv\"\n",
        "  )\n",
        "\n",
        "  if LOAD_MODEL:\n",
        "      # load_checkpoint(\n",
        "      #     CHECKPOINT_FILE, model, optimizer, LEARNING_RATE\n",
        "      # )\n",
        "      print(\"=> Loading checkpoint\")\n",
        "      checkpoint = torch.load(CHECKPOINT_FILE, map_location=DEVICE)\n",
        "      model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "      optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "  scaled_anchors = (\n",
        "      torch.tensor(ANCHORS)\n",
        "      * torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "  ).to(DEVICE)\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "      #plot_couple_examples(model, test_loader, 0.6, 0.5, scaled_anchors)\n",
        "      train(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
        "\n",
        "      if SAVE_MODEL:\n",
        "          #save_checkpoint(model, optimizer)\n",
        "          print(\"=> Saving checkpoint\")\n",
        "          checkpoint = {\n",
        "              \"state_dict\": model.state_dict(),\n",
        "              \"optimizer\": optimizer.state_dict(),\n",
        "          }\n",
        "          torch.save(checkpoint, filename)\n",
        "      \n",
        "\n",
        "      #print(f\"Currently epoch {epoch}\")\n",
        "      #print(\"On Train Eval loader:\")\n",
        "      #check_class_accuracy(model, train_eval_loader, threshold=config.CONF_THRESHOLD)\n",
        "      #print(\"On Train loader:\")\n",
        "      #check_class_accuracy(model, train_loader, threshold=config.CONF_THRESHOLD)\n",
        "\n",
        "      if epoch % 10 == 0 and epoch > 0:\n",
        "          print(\"On Test loader:\")\n",
        "          check_class_accuracy(model, test_loader, threshold=CONF_THRESHOLD)\n",
        "\n",
        "          pred_boxes, true_boxes = get_evaluation_bboxes(\n",
        "              test_loader,\n",
        "              model,\n",
        "              iou_threshold=NMS_IOU_THRESH,\n",
        "              anchors=ANCHORS,\n",
        "              threshold=CONF_THRESHOLD,\n",
        "          )\n",
        "          mapval = mean_average_precision(\n",
        "              pred_boxes,\n",
        "              true_boxes,\n",
        "              iou_threshold=MAP_IOU_THRESH,\n",
        "              box_format=\"midpoint\",\n",
        "              num_classes=NUM_CLASSES,\n",
        "          )\n",
        "          print(f\"MAP: {mapval.item()}\")\n"
      ],
      "id": "8d82f720-c8ee-48a5-be4d-4686e5b79474"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsHQvlchA1wB"
      },
      "outputs": [],
      "source": [
        "def get_evaluation_bboxes(\n",
        "  loader,\n",
        "  model,\n",
        "  iou_threshold,\n",
        "  anchors,\n",
        "  threshold,\n",
        "  box_format=\"midpoint\",\n",
        "  device=\"cuda\",\n",
        "):\n",
        "  # make sure model is in eval before get bboxes\n",
        "  model.eval()\n",
        "  train_idx = 0\n",
        "  all_pred_boxes = []\n",
        "  all_true_boxes = []\n",
        "  for batch_idx, (x, labels) in enumerate(tqdm(loader)):\n",
        "      x = x.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          predictions = model(x)\n",
        "\n",
        "      batch_size = x.shape[0]\n",
        "      bboxes = [[] for _ in range(batch_size)]\n",
        "      for i in range(3):\n",
        "          S = predictions[i].shape[2]\n",
        "          anchor = torch.tensor([*anchors[i]]).to(device) * S\n",
        "          boxes_scale_i = cells_to_bboxes(\n",
        "              predictions[i], anchor, S=S, is_preds=True\n",
        "          )\n",
        "          for idx, (box) in enumerate(boxes_scale_i):\n",
        "              bboxes[idx] += box\n",
        "\n",
        "      # we just want one bbox for each label, not one for each scale\n",
        "      true_bboxes = cells_to_bboxes(\n",
        "          labels[2], anchor, S=S, is_preds=False\n",
        "      )\n",
        "\n",
        "      for idx in range(batch_size):\n",
        "          nms_boxes = nms(\n",
        "              bboxes[idx],\n",
        "              iou_threshold=iou_threshold,\n",
        "              threshold=threshold,\n",
        "              box_format=box_format,\n",
        "          )\n",
        "\n",
        "          for nms_box in nms_boxes:\n",
        "              all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "          for box in true_bboxes[idx]:\n",
        "              if box[1] > threshold:\n",
        "                  all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "          train_idx += 1\n",
        "\n",
        "  model.train()\n",
        "  return all_pred_boxes, all_true_boxes\n"
      ],
      "id": "MsHQvlchA1wB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "8418a494-b9fa-4d2b-8a50-a724d51b95ed",
        "outputId": "6b336886-c0e7-4ea2-c377-88305ae6cf82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loading checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/518 [00:49<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-68fc34c81b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-a5ebd3c6be6e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#plot_couple_examples(model, test_loader, 0.6, 0.5, scaled_anchors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_anchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSAVE_MODEL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-0b05529fc998>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         y0, y1, y2 = (\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "main()"
      ],
      "id": "8418a494-b9fa-4d2b-8a50-a724d51b95ed"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gygCUwNEZNg"
      },
      "outputs": [],
      "source": [],
      "id": "7gygCUwNEZNg"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}