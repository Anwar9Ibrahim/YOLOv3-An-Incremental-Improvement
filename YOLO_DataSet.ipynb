{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YTtMmKdgQG4_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageFile\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "metadata": {
        "id": "iNoPnn5dQqmS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "MXku25IcQr2R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from intersection_over_union import intersection_over_union_wh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "zYbJNpc1QwZD",
        "outputId": "27447a87-ab67-41af-b167-8704596542e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-85ce69291d26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mintersection_over_union\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mintersection_over_union_wh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'intersection_over_union'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from non_max_suppression import nms\n"
      ],
      "metadata": {
        "id": "9SehJ636Qx-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#no errors while reading images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
      ],
      "metadata": {
        "id": "4cLgF5nfQ1Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#S is grid sizes\n",
        "#C number of classes\n",
        "class YOLODataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_file,\n",
        "        img_dir,\n",
        "        label_dir,\n",
        "        anchors,\n",
        "        image_size=416,\n",
        "        S=[13, 26, 52],\n",
        "        C=20,\n",
        "    ):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_size = image_size\n",
        "        self.S = S\n",
        "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
        "        self.num_anchors = self.anchors.shape[0]\n",
        "        self.num_anchors_per_scale = self.num_anchors // 3\n",
        "        self.C = C\n",
        "        self.ignore_iou_thresh = 0.5\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
        "        #bboxes = np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2).tolist() # [c,x,y,w,h]\n",
        "        #this one is for the agumentation using Albumentations lib\n",
        "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist() # [x,y,w,h,c]\n",
        "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "\n",
        "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
        "        targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]\n",
        "        #[3,13,13,[p_o , x,y ,w,h,c]]\n",
        "        # to specify which anchor box has the resbonsability for prediction in which scale\n",
        "        # we determain that by the anchor who has the highest IOU with the original box\n",
        "        for box in bboxes:\n",
        "            #print (self.anchors)\n",
        "            #print (torch.tensor(box[2:4]))\n",
        "            #iou_anchors = intersection_over_union(torch.tensor(box[2:4]), self.anchors)\n",
        "            iou_anchors = intersection_over_union_wh(torch.tensor(box[2:4]), self.anchors)\n",
        "            #iou_anchors = ops.box_iou(self.anchors,torch.tensor(box[2:4]))\n",
        "            #print (iou_anchors)\n",
        "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
        "            x, y, width, height, class_label = box\n",
        "            has_anchor = [False] * 3  # each scale should have one anchor\n",
        "            \n",
        "            for anchor_idx in anchor_indices:\n",
        "                #check which anchor belong to which scale\n",
        "                #scale_idx = anchor_idx // self.num_anchors_per_scale\n",
        "                scale_idx = torch.div(anchor_idx, self.num_anchors_per_scale, rounding_mode='floor')\n",
        "                \n",
        "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
        "                #how many cells in the particualr scale becuase the input in the labels is in the whole image but we want it to specific to the cell\n",
        "                #print (scale_idx)\n",
        "                S = self.S[scale_idx]\n",
        "                #which x and y cell\n",
        "                i, j = int(S * y), int(S * x) # x = 0.5 , S = 13 -> int(6.5) = 6\n",
        "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
        "                \n",
        "                #if the anchor is not taken in this specific scale and specific bounding box\n",
        "                if not anchor_taken and not has_anchor[scale_idx]:\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
        "                    # the cordination within the cell between [0,1]\n",
        "                    x_cell, y_cell = S * x - j, S * y - i  # s =13 ,x = 0.5-> 6.5 -6 = 0.5\n",
        "                    width_cell, height_cell = (\n",
        "                        width * S, #s=13 width = 0.5 , 6.5\n",
        "                        height * S,\n",
        "                    )  # can be greater than 1 since it's relative to cell\n",
        "                    box_coordinates = torch.tensor(\n",
        "                        [x_cell, y_cell, width_cell, height_cell]\n",
        "                    )\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
        "                    has_anchor[scale_idx] = True\n",
        "\n",
        "                # ignore other bounding boxes if they have an IOU > thresh but still there is a bounding box with a higher IOU for this scall\n",
        "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
        "\n",
        "        return image, tuple(targets)\n"
      ],
      "metadata": {
        "id": "4KJWYqgkQ5cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LtXP8QeRX-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}