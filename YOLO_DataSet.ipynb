{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTtMmKdgQG4_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from PIL import Image, ImageFile\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from intersection_over_union import intersection_over_union_wh\n",
        "from non_max_suppression import nms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cLgF5nfQ1Z8"
      },
      "outputs": [],
      "source": [
        "#no errors while reading images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KJWYqgkQ5cO"
      },
      "outputs": [],
      "source": [
        "#S is grid sizes\n",
        "#C number of classes\n",
        "class YOLODataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_file,\n",
        "        img_dir,\n",
        "        label_dir,\n",
        "        anchors,\n",
        "        image_size=416,\n",
        "        S=[13, 26, 52],\n",
        "        C=20,\n",
        "        transform = None\n",
        "    ):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_size = image_size\n",
        "        self.S = S\n",
        "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
        "        self.num_anchors = self.anchors.shape[0]\n",
        "        self.num_anchors_per_scale = self.num_anchors // 3\n",
        "        self.C = C\n",
        "        self.ignore_iou_thresh = 0.5\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
        "        #bboxes = np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2).tolist() # [c,x,y,w,h]\n",
        "        #this one is for the agumentation using Albumentations lib\n",
        "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist() # [x,y,w,h,c]\n",
        "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "        if self.transform:\n",
        "            augmentations = self.transform(image=image, bboxes=bboxes)\n",
        "            image = augmentations[\"image\"]\n",
        "            bboxes = augmentations[\"bboxes\"]\n",
        "\n",
        "\n",
        "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
        "        targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]\n",
        "        #[3,13,13,[p_o , x,y ,w,h,c]]\n",
        "        # to specify which anchor box has the resbonsability for prediction in which scale\n",
        "        # we determain that by the anchor who has the highest IOU with the original box\n",
        "        for box in bboxes:\n",
        "            #print (self.anchors)\n",
        "            #print (torch.tensor(box[2:4]))\n",
        "            #iou_anchors = intersection_over_union(torch.tensor(box[2:4]), self.anchors)\n",
        "            iou_anchors = intersection_over_union_wh(torch.tensor(box[2:4]), self.anchors)\n",
        "            #iou_anchors = ops.box_iou(self.anchors,torch.tensor(box[2:4]))\n",
        "            #print (iou_anchors)\n",
        "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
        "            x, y, width, height, class_label = box\n",
        "            has_anchor = [False] * 3  # each scale should have one anchor\n",
        "            \n",
        "            for anchor_idx in anchor_indices:\n",
        "                #check which anchor belong to which scale\n",
        "                #scale_idx = anchor_idx // self.num_anchors_per_scale\n",
        "                scale_idx = torch.div(anchor_idx, self.num_anchors_per_scale, rounding_mode='floor')\n",
        "                \n",
        "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
        "                #how many cells in the particualr scale becuase the input in the labels is in the whole image but we want it to specific to the cell\n",
        "                #print (scale_idx)\n",
        "                S = self.S[scale_idx]\n",
        "                #which x and y cell\n",
        "                i, j = int(S * y), int(S * x) # x = 0.5 , S = 13 -> int(6.5) = 6\n",
        "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
        "                \n",
        "                #if the anchor is not taken in this specific scale and specific bounding box\n",
        "                if not anchor_taken and not has_anchor[scale_idx]:\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
        "                    # the cordination within the cell between [0,1]\n",
        "                    x_cell, y_cell = S * x - j, S * y - i  # s =13 ,x = 0.5-> 6.5 -6 = 0.5\n",
        "                    width_cell, height_cell = (\n",
        "                        width * S, #s=13 width = 0.5 , 6.5\n",
        "                        height * S,\n",
        "                    )  # can be greater than 1 since it's relative to cell\n",
        "                    box_coordinates = torch.tensor(\n",
        "                        [x_cell, y_cell, width_cell, height_cell]\n",
        "                    )\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
        "                    has_anchor[scale_idx] = True\n",
        "\n",
        "                # ignore other bounding boxes if they have an IOU > thresh but still there is a bounding box with a higher IOU for this scall\n",
        "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
        "\n",
        "        return image, tuple(targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dtIyKn5agCju"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}